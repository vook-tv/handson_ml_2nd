{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "13장 텐서플로에서 데이터 적재와 전처리하기.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "97db3f173b9f4bcb84a3c7745eb2c05f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_11e23f1329d646bca04a17052b958480",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ac1292c4ed5745a480bc6bdbde16cdae",
              "IPY_MODEL_4d98c06d7300431ea903cf9303f757ec"
            ]
          }
        },
        "11e23f1329d646bca04a17052b958480": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ac1292c4ed5745a480bc6bdbde16cdae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f50ed3baa45b413abe9c7e716a926ad2",
            "_dom_classes": [],
            "description": "Dl Completed...: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a858cf5fe291421488eb028914ad48a1"
          }
        },
        "4d98c06d7300431ea903cf9303f757ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_87a80960e4114945a9be76359dfdcbe3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4/4 [00:00&lt;00:00,  5.61 file/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6050d2c8611b427691cb4f952ed5dc52"
          }
        },
        "f50ed3baa45b413abe9c7e716a926ad2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a858cf5fe291421488eb028914ad48a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "87a80960e4114945a9be76359dfdcbe3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6050d2c8611b427691cb4f952ed5dc52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPB3LLlJvKy7",
        "outputId": "20b43ea6-fecf-4824-bfb0-8829190bf927"
      },
      "source": [
        "# 일단 import \n",
        "%matplotlib inline  \n",
        "\n",
        "import matplotlib as mpl  # 기본 설정 만지는 용도\n",
        "import matplotlib.pyplot as plt  # 그래프 그리는 용도\n",
        "import matplotlib.font_manager as fm  # 폰트 관련 용도\n",
        "\n",
        "\n",
        "print(mpl.__version__)\n",
        "print(mpl.__file__)\n",
        "print(mpl.get_configdir())\n",
        "print(mpl.get_cachedir())\n",
        "\n",
        "sys_font=fm.findSystemFonts()\n",
        "print(f\"sys_font number: {len(sys_font)}\")\n",
        "print(sys_font)\n",
        "\n",
        "nanum_font = [f for f in sys_font if 'Nanum' in f]\n",
        "print(f\"nanum_font number: {len(nanum_font)}\")\n",
        "\n",
        "!apt-get update -qq\n",
        "!apt-get install fonts-nanum* -qq\n",
        "\n",
        "\n",
        "# 체크해보면 폰트 개수가 늘어났다\n",
        "sys_font=fm.findSystemFonts()\n",
        "print(f\"sys_font number: {len(sys_font)}\")\n",
        "\n",
        "nanum_font = [f for f in sys_font if 'Nanum' in f]\n",
        "print(f\"nanum_font number: {len(nanum_font)}\")\n",
        "\n",
        "\n",
        "# 현재 설정되어 있는 폰트 사이즈와 글꼴을 알아보자\n",
        "!python --version\n",
        "def current_font():\n",
        "  print(f\"설정 폰트 글꼴: {plt.rcParams['font.family']}, 설정 폰트 사이즈: {plt.rcParams['font.size']}\")  # 파이썬 3.6 이상 사용가능하다\n",
        "        \n",
        "current_font()\n",
        "\n",
        "path = '/usr/share/fonts/truetype/nanum/NanumGothicEco.ttf'  # 설치된 나눔글꼴중 원하는 녀석의 전체 경로를 가져오자\n",
        "font_name = fm.FontProperties(fname=path, size=10).get_name()\n",
        "print(font_name)\n",
        "plt.rc('font', family=font_name)\n",
        "\n",
        "# 우선 fm._rebuild() 를 해주고\n",
        "fm._rebuild()\n",
        "\n",
        "mpl.rcParams['axes.unicode_minus'] = False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.2.2\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\n",
            "/root/.config/matplotlib\n",
            "/root/.cache/matplotlib\n",
            "sys_font number: 17\n",
            "['/usr/share/fonts/truetype/liberation/LiberationSerif-Regular.ttf', '/usr/share/fonts/truetype/liberation/LiberationMono-Regular.ttf', '/usr/share/fonts/truetype/liberation/LiberationSans-BoldItalic.ttf', '/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Bold.ttf', '/usr/share/fonts/truetype/liberation/LiberationMono-BoldItalic.ttf', '/usr/share/fonts/truetype/liberation/LiberationMono-Bold.ttf', '/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Italic.ttf', '/usr/share/fonts/truetype/liberation/LiberationMono-Italic.ttf', '/usr/share/fonts/truetype/liberation/LiberationSans-Italic.ttf', '/usr/share/fonts/truetype/liberation/LiberationSerif-BoldItalic.ttf', '/usr/share/fonts/truetype/liberation/LiberationSansNarrow-BoldItalic.ttf', '/usr/share/fonts/truetype/liberation/LiberationSerif-Italic.ttf', '/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Regular.ttf', '/usr/share/fonts/truetype/humor-sans/Humor-Sans.ttf', '/usr/share/fonts/truetype/liberation/LiberationSerif-Bold.ttf', '/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf', '/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf']\n",
            "nanum_font number: 0\n",
            "Selecting previously unselected package fonts-nanum.\n",
            "(Reading database ... 144793 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-nanum_20170925-1_all.deb ...\n",
            "Unpacking fonts-nanum (20170925-1) ...\n",
            "Selecting previously unselected package fonts-nanum-eco.\n",
            "Preparing to unpack .../fonts-nanum-eco_1.000-6_all.deb ...\n",
            "Unpacking fonts-nanum-eco (1.000-6) ...\n",
            "Selecting previously unselected package fonts-nanum-extra.\n",
            "Preparing to unpack .../fonts-nanum-extra_20170925-1_all.deb ...\n",
            "Unpacking fonts-nanum-extra (20170925-1) ...\n",
            "Selecting previously unselected package fonts-nanum-coding.\n",
            "Preparing to unpack .../fonts-nanum-coding_2.5-1_all.deb ...\n",
            "Unpacking fonts-nanum-coding (2.5-1) ...\n",
            "Setting up fonts-nanum-extra (20170925-1) ...\n",
            "Setting up fonts-nanum (20170925-1) ...\n",
            "Setting up fonts-nanum-coding (2.5-1) ...\n",
            "Setting up fonts-nanum-eco (1.000-6) ...\n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n",
            "sys_font number: 48\n",
            "nanum_font number: 31\n",
            "Python 3.6.9\n",
            "설정 폰트 글꼴: ['sans-serif'], 설정 폰트 사이즈: 10.0\n",
            "NanumGothic Eco\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XG0HdN-FvQ0t",
        "outputId": "82deb82b-8ea2-4f0f-817f-8c0335b54a54"
      },
      "source": [
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "try:\n",
        "  !pip install -a -U tfx==0.21.2\n",
        "  print('error')\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc(\"axes\", labelsize=14)\n",
        "mpl.rc(\"xtick\", labelsize=12)\n",
        "mpl.rc(\"ytick\", labelsize=12)\n",
        "\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"data\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "  path= os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "  print(\"save figure\", fig_id)\n",
        "  if tight_layout:\n",
        "    plt.tight_layout()\n",
        "  plt.savefig(path, format=fig_extension, dpi=resolution)\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Usage:   \n",
            "  pip3 install [options] <requirement specifier> [package-index-options] ...\n",
            "  pip3 install [options] -r <requirements file> [package-index-options] ...\n",
            "  pip3 install [options] [-e] <vcs project url> ...\n",
            "  pip3 install [options] [-e] <local project path> ...\n",
            "  pip3 install [options] <archive url/path> ...\n",
            "\n",
            "no such option: -a\n",
            "error\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQrsA4wAw4ap",
        "outputId": "95f36d04-1bf5-411a-a33a-eb7872fcdbaf"
      },
      "source": [
        "X = tf.range(10) #샘플 데이터 텐서\n",
        "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TensorSliceDataset shapes: (), types: tf.int32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oONCpzd3w_Rx",
        "outputId": "bae5ff5b-99d3-4890-c248-948e98e72286"
      },
      "source": [
        "for item in dataset:\n",
        "  print(item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0, shape=(), dtype=int32)\n",
            "tf.Tensor(1, shape=(), dtype=int32)\n",
            "tf.Tensor(2, shape=(), dtype=int32)\n",
            "tf.Tensor(3, shape=(), dtype=int32)\n",
            "tf.Tensor(4, shape=(), dtype=int32)\n",
            "tf.Tensor(5, shape=(), dtype=int32)\n",
            "tf.Tensor(6, shape=(), dtype=int32)\n",
            "tf.Tensor(7, shape=(), dtype=int32)\n",
            "tf.Tensor(8, shape=(), dtype=int32)\n",
            "tf.Tensor(9, shape=(), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8TNmkCRxF6v"
      },
      "source": [
        "13.1.1 연쇄 변환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXmbX6tKxH-P"
      },
      "source": [
        "dataset = dataset.repeat(3).batch(7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ew0YurozxLcB",
        "outputId": "5e2fa582-38fd-49e1-d497-124a2326f4e8"
      },
      "source": [
        "for item in dataset:\n",
        "  print(item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
            "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
            "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
            "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
            "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvuM3jSKxNsR"
      },
      "source": [
        "dataset = dataset.map(lambda x: x * 2) # 아이템 [0,2,4,6,8,10,12]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkMxHe4RxVIx",
        "outputId": "d5cb8a2e-abc3-4f1f-cb9c-59f86bb9adb0"
      },
      "source": [
        "for item in dataset:\n",
        "  print(item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int32)\n",
            "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
            "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n",
            "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)\n",
            "tf.Tensor([16 18], shape=(2,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdhsEhfkxbjD",
        "outputId": "ee3d2228-4798-4a83-c985-c735ecd70250"
      },
      "source": [
        "dataset = dataset.apply(tf.data.experimental.unbatch()) #아이템 : 0,2,4....."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-9-d715b57de17b>:1: unbatch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.unbatch()`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EslviWWsxoQP"
      },
      "source": [
        "dataset = dataset.filter(lambda x: x < 10) #아이템: 0 2 4 6 8 0 2 4 6 ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebU3Om-0xucO",
        "outputId": "eeeaa16b-0ad2-4916-91f3-50a25e569129"
      },
      "source": [
        "for item in dataset:\n",
        "  print(item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0, shape=(), dtype=int32)\n",
            "tf.Tensor(2, shape=(), dtype=int32)\n",
            "tf.Tensor(4, shape=(), dtype=int32)\n",
            "tf.Tensor(6, shape=(), dtype=int32)\n",
            "tf.Tensor(8, shape=(), dtype=int32)\n",
            "tf.Tensor(0, shape=(), dtype=int32)\n",
            "tf.Tensor(2, shape=(), dtype=int32)\n",
            "tf.Tensor(4, shape=(), dtype=int32)\n",
            "tf.Tensor(6, shape=(), dtype=int32)\n",
            "tf.Tensor(8, shape=(), dtype=int32)\n",
            "tf.Tensor(0, shape=(), dtype=int32)\n",
            "tf.Tensor(2, shape=(), dtype=int32)\n",
            "tf.Tensor(4, shape=(), dtype=int32)\n",
            "tf.Tensor(6, shape=(), dtype=int32)\n",
            "tf.Tensor(8, shape=(), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9pizVcUxvyq"
      },
      "source": [
        "13.1.2 데이터 셔플링"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4JjqRXdxyTj"
      },
      "source": [
        "dataset = tf.data.Dataset.range(10).repeat(3) # 0에서 9까지 3번 반복\n",
        "dataset = dataset.shuffle(buffer_size=5, seed=42).batch(7)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rin73Cdfx-gi",
        "outputId": "b045114a-7d94-4a6e-a8be-12dd6054fda4"
      },
      "source": [
        "for item in dataset:\n",
        "  print(item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([0 2 3 6 7 9 4], shape=(7,), dtype=int64)\n",
            "tf.Tensor([5 0 1 1 8 6 5], shape=(7,), dtype=int64)\n",
            "tf.Tensor([4 8 7 1 2 3 0], shape=(7,), dtype=int64)\n",
            "tf.Tensor([5 4 2 7 8 9 9], shape=(7,), dtype=int64)\n",
            "tf.Tensor([3 6], shape=(2,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSlvYgOix_nE",
        "outputId": "5f9d8a7f-90c3-4c6e-a939-fffa19697cf7"
      },
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "x_train_full, x_test, y_train_full, y_test = train_test_split(\n",
        "    housing.data, housing.target.reshape(-1, 1), random_state=42\n",
        ")\n",
        "\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(\n",
        "    x_train_full, y_train_full, random_state=42\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(x_train)\n",
        "x_mean = scaler.mean_\n",
        "x_std = scaler.scale_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /root/scikit_learn_data\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwTRdCoZy005"
      },
      "source": [
        "def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n",
        "  housing_dir = os.path.join(\"datasets\", \"housing\")\n",
        "  os.makedirs(housing_dir, exist_ok=True)\n",
        "  path_format = os.path.join(housing_dir, \"my_{}_{:02d}.csv\")\n",
        "\n",
        "  filepaths = []\n",
        "  m = len(data)\n",
        "  for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n",
        "    part_csv = path_format.format(name_prefix, file_idx)\n",
        "    filepaths.append(part_csv)\n",
        "    with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
        "      if header is not None:\n",
        "        f.write(header)\n",
        "        f.write(\"\\n\")\n",
        "      for row_idx in row_indices:\n",
        "        f.write(\",\".join(\n",
        "            [repr(col) for col in data[row_idx]]\n",
        "        ))\n",
        "        f.write(\"\\n\")\n",
        "  return filepaths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYNXAdgUzqPp"
      },
      "source": [
        "train_data = np.c_[x_train, y_train]\n",
        "valid_data = np.c_[x_valid, y_valid]\n",
        "test_data = np.c_[x_test, y_test]\n",
        "header_cols = housing.feature_names + [\"MedianHouseValue\"]\n",
        "header = \",\".join(header_cols)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOQQpZ-U0Agm"
      },
      "source": [
        "train_filepaths = save_to_multiple_csv_files(train_data, \"train\", header, n_parts=20)\n",
        "valid_filepaths = save_to_multiple_csv_files(valid_data, \"valid\", header, n_parts=10)\n",
        "test_filepaths = save_to_multiple_csv_files(test_data, \"test\", header, n_parts=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "RR4PI9Tu0Mca",
        "outputId": "d5f2b8a0-eadc-481e-e77d-8a73b857e87c"
      },
      "source": [
        "import pandas as pd\n",
        "pd.read_csv( train_filepaths[0] ).head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MedInc</th>\n",
              "      <th>HouseAge</th>\n",
              "      <th>AveRooms</th>\n",
              "      <th>AveBedrms</th>\n",
              "      <th>Population</th>\n",
              "      <th>AveOccup</th>\n",
              "      <th>Latitude</th>\n",
              "      <th>Longitude</th>\n",
              "      <th>MedianHouseValue</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.5214</td>\n",
              "      <td>15.0</td>\n",
              "      <td>3.049945</td>\n",
              "      <td>1.106548</td>\n",
              "      <td>1447.0</td>\n",
              "      <td>1.605993</td>\n",
              "      <td>37.63</td>\n",
              "      <td>-122.43</td>\n",
              "      <td>1.442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.3275</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6.490060</td>\n",
              "      <td>0.991054</td>\n",
              "      <td>3464.0</td>\n",
              "      <td>3.443340</td>\n",
              "      <td>33.69</td>\n",
              "      <td>-117.39</td>\n",
              "      <td>1.687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.1000</td>\n",
              "      <td>29.0</td>\n",
              "      <td>7.542373</td>\n",
              "      <td>1.591525</td>\n",
              "      <td>1328.0</td>\n",
              "      <td>2.250847</td>\n",
              "      <td>38.44</td>\n",
              "      <td>-122.98</td>\n",
              "      <td>1.621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7.1736</td>\n",
              "      <td>12.0</td>\n",
              "      <td>6.289003</td>\n",
              "      <td>0.997442</td>\n",
              "      <td>1054.0</td>\n",
              "      <td>2.695652</td>\n",
              "      <td>33.55</td>\n",
              "      <td>-117.70</td>\n",
              "      <td>2.621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0549</td>\n",
              "      <td>13.0</td>\n",
              "      <td>5.312457</td>\n",
              "      <td>1.085092</td>\n",
              "      <td>3297.0</td>\n",
              "      <td>2.244384</td>\n",
              "      <td>33.93</td>\n",
              "      <td>-116.93</td>\n",
              "      <td>0.956</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   MedInc  HouseAge  AveRooms  ...  Latitude  Longitude  MedianHouseValue\n",
              "0  3.5214      15.0  3.049945  ...     37.63    -122.43             1.442\n",
              "1  5.3275       5.0  6.490060  ...     33.69    -117.39             1.687\n",
              "2  3.1000      29.0  7.542373  ...     38.44    -122.98             1.621\n",
              "3  7.1736      12.0  6.289003  ...     33.55    -117.70             2.621\n",
              "4  2.0549      13.0  5.312457  ...     33.93    -116.93             0.956\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlWzQFKr0Sz3",
        "outputId": "75e30858-0b4a-46c9-fbba-069663fc30d6"
      },
      "source": [
        "with open(train_filepaths[0]) as f:\n",
        "  for i in range(5):\n",
        "    print(f.readline(), end=\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n",
            "3.5214,15.0,3.0499445061043287,1.106548279689234,1447.0,1.6059933407325193,37.63,-122.43,1.442\n",
            "5.3275,5.0,6.490059642147117,0.9910536779324056,3464.0,3.4433399602385686,33.69,-117.39,1.687\n",
            "3.1,29.0,7.5423728813559325,1.5915254237288134,1328.0,2.2508474576271187,38.44,-122.98,1.621\n",
            "7.1736,12.0,6.289002557544757,0.9974424552429667,1054.0,2.6956521739130435,33.55,-117.7,2.621\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWwQgU_D0dij",
        "outputId": "ac356592-469a-4b8f-d972-8eaad51e311c"
      },
      "source": [
        "train_filepaths"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['datasets/housing/my_train_00.csv',\n",
              " 'datasets/housing/my_train_01.csv',\n",
              " 'datasets/housing/my_train_02.csv',\n",
              " 'datasets/housing/my_train_03.csv',\n",
              " 'datasets/housing/my_train_04.csv',\n",
              " 'datasets/housing/my_train_05.csv',\n",
              " 'datasets/housing/my_train_06.csv',\n",
              " 'datasets/housing/my_train_07.csv',\n",
              " 'datasets/housing/my_train_08.csv',\n",
              " 'datasets/housing/my_train_09.csv',\n",
              " 'datasets/housing/my_train_10.csv',\n",
              " 'datasets/housing/my_train_11.csv',\n",
              " 'datasets/housing/my_train_12.csv',\n",
              " 'datasets/housing/my_train_13.csv',\n",
              " 'datasets/housing/my_train_14.csv',\n",
              " 'datasets/housing/my_train_15.csv',\n",
              " 'datasets/housing/my_train_16.csv',\n",
              " 'datasets/housing/my_train_17.csv',\n",
              " 'datasets/housing/my_train_18.csv',\n",
              " 'datasets/housing/my_train_19.csv']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xhw_nR1U0fYl"
      },
      "source": [
        "filepath_dataset =  tf.data.Dataset.list_files(train_filepaths, seed=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENp5q4_l0mC6",
        "outputId": "42ee07a8-c201-4ffb-ad19-9ba1f8d111e7"
      },
      "source": [
        "for filepath in filepath_dataset:\n",
        "  print(filepath)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(b'datasets/housing/my_train_05.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_16.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_01.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_17.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_00.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_14.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_10.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_02.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_12.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_19.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_07.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_09.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_13.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_15.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_11.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_18.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_04.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_06.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_03.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_08.csv', shape=(), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLhJE7sJ0pc5"
      },
      "source": [
        "n_readers = 5\n",
        "dataset = filepath_dataset.interleave(\n",
        "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
        "    cycle_length=n_readers\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_j6d--P00GU",
        "outputId": "75c3730d-0182-4d46-89f8-f0dd768f516e"
      },
      "source": [
        "for line in dataset.take(5):\n",
        "  print(line.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'4.5909,16.0,5.475877192982456,1.0964912280701755,1357.0,2.9758771929824563,33.63,-117.71,2.418'\n",
            "b'2.4792,24.0,3.4547038327526134,1.1341463414634145,2251.0,3.921602787456446,34.18,-118.38,2.0'\n",
            "b'4.2708,45.0,5.121387283236994,0.953757225433526,492.0,2.8439306358381504,37.48,-122.19,2.67'\n",
            "b'2.1856,41.0,3.7189873417721517,1.0658227848101265,803.0,2.0329113924050635,32.76,-117.12,1.205'\n",
            "b'4.1812,52.0,5.701388888888889,0.9965277777777778,692.0,2.4027777777777777,33.73,-118.31,3.215'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOkQykkk04dt"
      },
      "source": [
        "13.1.3 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1cLKjvs07Hg"
      },
      "source": [
        "#x_mean, x_std = [...] 훈련 세트에 있는 각 특성의 평균과 표준 편차\n",
        "n_inputs = 8\n",
        "\n",
        "def preprocess(line):\n",
        "  defs = [0.] * n_inputs + [tf.constant(\n",
        "      [], dtype=tf.float32\n",
        "  )]\n",
        "  fields = tf.io.decode_csv(line, record_defaults=defs)\n",
        "  x = tf.stack(fields[:-1])\n",
        "  y = tf.stack(fields[-1:])\n",
        "  return (x - x_mean) / x_std, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-gTYPnk41K7",
        "outputId": "11a75b41-171d-4328-e623-72ff5578ce43"
      },
      "source": [
        "preprocess(b'4.2083,44.0, 5.3232, 0.9171,846.0, 2.3370,37.47, -122.2, 2.782')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
              " array([ 0.16579159,  1.216324  , -0.05204564, -0.39215982, -0.5277444 ,\n",
              "        -0.2633488 ,  0.8543046 , -1.3072058 ], dtype=float32)>,\n",
              " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.782], dtype=float32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8SYuq3z5AVe"
      },
      "source": [
        "13.1.4 데이터 적재와 전처리를 합치기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5WfDAe-5Gi0"
      },
      "source": [
        "def csv_reader_dataset(filepaths, repeat=1, n_readers=5, n_read_threads=None,\n",
        "                       shuffle_buffer_size=10000, n_parse_threads=5, batch_size=32):\n",
        "  dataset = tf.data.Dataset.list_files(filepaths).repeat(repeat)\n",
        "  dataset = dataset.interleave(\n",
        "      lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
        "      cycle_length=n_readers, num_parallel_calls=n_read_threads\n",
        "  )\n",
        "\n",
        "  dataset = dataset.shuffle(shuffle_buffer_size)\n",
        "  dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
        "  return dataset.batch(batch_size).prefetch(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3vcU6QD5rxK",
        "outputId": "24861897-99be-4eaa-ceba-8bd72a6bcba5"
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "train_set = csv_reader_dataset(train_filepaths, batch_size=3)\n",
        "for x_batch, y_batch in train_set.take(2):\n",
        "  print(\"x =\", x_batch)\n",
        "  print(\"y =\", y_batch)\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x = tf.Tensor(\n",
            "[[ 0.5804519  -0.20762321  0.05616303 -0.15191229  0.01343246  0.00604472\n",
            "   1.2525111  -1.3671792 ]\n",
            " [ 5.818099    1.8491895   1.1784915   0.28173092 -1.2496178  -0.3571987\n",
            "   0.7231292  -1.0023477 ]\n",
            " [-0.9253566   0.5834586  -0.7807257  -0.28213993 -0.36530012  0.27389365\n",
            "  -0.76194876  0.72684526]], shape=(3, 8), dtype=float32)\n",
            "y = tf.Tensor(\n",
            "[[1.752]\n",
            " [1.313]\n",
            " [1.535]], shape=(3, 1), dtype=float32)\n",
            "\n",
            "x = tf.Tensor(\n",
            "[[-0.8324941   0.6625668  -0.20741376 -0.18699841 -0.14536144  0.09635526\n",
            "   0.9807942  -0.67250353]\n",
            " [-0.62183803  0.5834586  -0.19862501 -0.3500319  -1.1437552  -0.3363751\n",
            "   1.107282   -0.8674123 ]\n",
            " [ 0.8683102   0.02970133  0.3427381  -0.29872298  0.7124906   0.28026953\n",
            "  -0.72915536  0.86178064]], shape=(3, 8), dtype=float32)\n",
            "y = tf.Tensor(\n",
            "[[0.919]\n",
            " [1.028]\n",
            " [2.182]], shape=(3, 1), dtype=float32)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9gvX7zT58-5"
      },
      "source": [
        "13.1.5 프리페치"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2JnW6-86BJJ"
      },
      "source": [
        "13.1.6 tf.keras와 데이터셋 사용하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DcIMa0c6E2W"
      },
      "source": [
        "train_set = csv_reader_dataset(train_filepaths)\n",
        "valid_set = csv_reader_dataset(valid_filepaths)\n",
        "test_set = csv_reader_dataset(test_filepaths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpR6IM9G6Mxp"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pO8MT8cU6Uxj"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "  keras.layers.Dense(30, activation=\"relu\", input_shape=x_train.shape[1:]),\n",
        "  keras.layers.Dense(1)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUqWtvcI6hPb"
      },
      "source": [
        "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saYYNzXs6oCt",
        "outputId": "1b77558d-4ec5-44cc-fe37-b8d6b58db60c"
      },
      "source": [
        "batch_size = 32\n",
        "model.fit(train_set, epochs=10, validation_data=valid_set) #steps_per_epoch=len(x_train) // batch_size, "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "363/363 [==============================] - 1s 3ms/step - loss: 1.6391 - val_loss: 0.9463\n",
            "Epoch 2/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.7066 - val_loss: 0.6542\n",
            "Epoch 3/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.6346 - val_loss: 0.6065\n",
            "Epoch 4/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5977 - val_loss: 0.5705\n",
            "Epoch 5/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5701 - val_loss: 0.5420\n",
            "Epoch 6/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5477 - val_loss: 0.5275\n",
            "Epoch 7/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5294 - val_loss: 0.4944\n",
            "Epoch 8/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5130 - val_loss: 0.4850\n",
            "Epoch 9/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4997 - val_loss: 0.4670\n",
            "Epoch 10/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4879 - val_loss: 0.4587\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fce315aa7b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4LRGySa64Kx",
        "outputId": "a08fde2a-89b1-4ec0-eaf4-4263919b4fa2"
      },
      "source": [
        "model.evaluate(test_set)\n",
        "new_set = test_set.take(3).map(lambda X, y: X) #새로운 샘플이 3개 있다고 가정\n",
        "model.predict(new_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "162/162 [==============================] - 0s 2ms/step - loss: 0.4768\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.8246477 ],\n",
              "       [1.1649357 ],\n",
              "       [1.7166657 ],\n",
              "       [0.97491825],\n",
              "       [2.4979835 ],\n",
              "       [2.2972643 ],\n",
              "       [1.7556832 ],\n",
              "       [4.3428483 ],\n",
              "       [2.060362  ],\n",
              "       [2.3412867 ],\n",
              "       [2.4690478 ],\n",
              "       [2.961075  ],\n",
              "       [2.7318287 ],\n",
              "       [2.3811243 ],\n",
              "       [1.1390718 ],\n",
              "       [2.0844975 ],\n",
              "       [3.519305  ],\n",
              "       [2.28826   ],\n",
              "       [4.907612  ],\n",
              "       [1.7516866 ],\n",
              "       [4.2348604 ],\n",
              "       [1.2620912 ],\n",
              "       [2.38176   ],\n",
              "       [1.577371  ],\n",
              "       [2.79051   ],\n",
              "       [1.756511  ],\n",
              "       [1.794009  ],\n",
              "       [2.4247792 ],\n",
              "       [3.114032  ],\n",
              "       [1.5022981 ],\n",
              "       [2.2059555 ],\n",
              "       [1.3772786 ],\n",
              "       [2.3967018 ],\n",
              "       [1.5629871 ],\n",
              "       [5.516988  ],\n",
              "       [2.2404964 ],\n",
              "       [1.8688171 ],\n",
              "       [2.1682742 ],\n",
              "       [1.3639779 ],\n",
              "       [3.1941426 ],\n",
              "       [1.1026578 ],\n",
              "       [1.0875453 ],\n",
              "       [3.9721904 ],\n",
              "       [1.987387  ],\n",
              "       [1.3666742 ],\n",
              "       [2.9993486 ],\n",
              "       [2.2836924 ],\n",
              "       [1.0508659 ],\n",
              "       [1.3058462 ],\n",
              "       [1.3596747 ],\n",
              "       [1.9115155 ],\n",
              "       [2.350611  ],\n",
              "       [2.6670456 ],\n",
              "       [1.6383772 ],\n",
              "       [2.2992575 ],\n",
              "       [0.99540925],\n",
              "       [2.359653  ],\n",
              "       [1.2289126 ],\n",
              "       [1.5455941 ],\n",
              "       [5.3019314 ],\n",
              "       [4.4962535 ],\n",
              "       [3.5361636 ],\n",
              "       [2.436297  ],\n",
              "       [1.9205153 ],\n",
              "       [2.5120163 ],\n",
              "       [0.7248587 ],\n",
              "       [1.573484  ],\n",
              "       [2.9770164 ],\n",
              "       [1.271822  ],\n",
              "       [1.8923371 ],\n",
              "       [1.8539832 ],\n",
              "       [1.8537922 ],\n",
              "       [1.7819827 ],\n",
              "       [2.3288326 ],\n",
              "       [1.2874515 ],\n",
              "       [1.4455745 ],\n",
              "       [0.8912621 ],\n",
              "       [1.346461  ],\n",
              "       [1.204444  ],\n",
              "       [3.2217157 ],\n",
              "       [3.75526   ],\n",
              "       [2.3394845 ],\n",
              "       [2.1448634 ],\n",
              "       [1.8291535 ],\n",
              "       [2.4989488 ],\n",
              "       [1.2500322 ],\n",
              "       [2.233777  ],\n",
              "       [2.1548967 ],\n",
              "       [2.611489  ],\n",
              "       [3.1035419 ],\n",
              "       [5.7782917 ],\n",
              "       [2.0553198 ],\n",
              "       [2.9734356 ],\n",
              "       [3.4703932 ],\n",
              "       [4.2141747 ],\n",
              "       [1.876009  ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rM10BLL7jjW",
        "outputId": "107a69d0-22b0-49cb-841a-56fa173fd005"
      },
      "source": [
        "optimizer = keras.optimizers.Nadam(lr=0.01)\n",
        "loss_fn = keras.losses.mean_squared_error\n",
        "n_epochs = 5\n",
        "batch_size = 32\n",
        "n_steps_per_epoch = len(x_train) // batch_size\n",
        "total_steps = n_epochs * n_steps_per_epoch\n",
        "global_step = 0\n",
        "\n",
        "#for x_batch, y_batch in train_set:\n",
        "for x_batch, y_batch in train_set.take(total_steps):\n",
        "  global_step += 1\n",
        "  print(\"\\n Global step {}/{}\".format(global_step, total_steps), end=\"\")\n",
        "  with tf.GradientTape() as tape:\n",
        "    y_pred = model(x_batch)\n",
        "    main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
        "    loss = tf.add_n(\n",
        "        [main_loss] + model.losses\n",
        "    )\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(\n",
        "      gradients, model.trainable_variables\n",
        "  ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Global step 1/1810\n",
            " Global step 2/1810\n",
            " Global step 3/1810\n",
            " Global step 4/1810\n",
            " Global step 5/1810\n",
            " Global step 6/1810\n",
            " Global step 7/1810\n",
            " Global step 8/1810\n",
            " Global step 9/1810\n",
            " Global step 10/1810\n",
            " Global step 11/1810\n",
            " Global step 12/1810\n",
            " Global step 13/1810\n",
            " Global step 14/1810\n",
            " Global step 15/1810\n",
            " Global step 16/1810\n",
            " Global step 17/1810\n",
            " Global step 18/1810\n",
            " Global step 19/1810\n",
            " Global step 20/1810\n",
            " Global step 21/1810\n",
            " Global step 22/1810\n",
            " Global step 23/1810\n",
            " Global step 24/1810\n",
            " Global step 25/1810\n",
            " Global step 26/1810\n",
            " Global step 27/1810\n",
            " Global step 28/1810\n",
            " Global step 29/1810\n",
            " Global step 30/1810\n",
            " Global step 31/1810\n",
            " Global step 32/1810\n",
            " Global step 33/1810\n",
            " Global step 34/1810\n",
            " Global step 35/1810\n",
            " Global step 36/1810\n",
            " Global step 37/1810\n",
            " Global step 38/1810\n",
            " Global step 39/1810\n",
            " Global step 40/1810\n",
            " Global step 41/1810\n",
            " Global step 42/1810\n",
            " Global step 43/1810\n",
            " Global step 44/1810\n",
            " Global step 45/1810\n",
            " Global step 46/1810\n",
            " Global step 47/1810\n",
            " Global step 48/1810\n",
            " Global step 49/1810\n",
            " Global step 50/1810\n",
            " Global step 51/1810\n",
            " Global step 52/1810\n",
            " Global step 53/1810\n",
            " Global step 54/1810\n",
            " Global step 55/1810\n",
            " Global step 56/1810\n",
            " Global step 57/1810\n",
            " Global step 58/1810\n",
            " Global step 59/1810\n",
            " Global step 60/1810\n",
            " Global step 61/1810\n",
            " Global step 62/1810\n",
            " Global step 63/1810\n",
            " Global step 64/1810\n",
            " Global step 65/1810\n",
            " Global step 66/1810\n",
            " Global step 67/1810\n",
            " Global step 68/1810\n",
            " Global step 69/1810\n",
            " Global step 70/1810\n",
            " Global step 71/1810\n",
            " Global step 72/1810\n",
            " Global step 73/1810\n",
            " Global step 74/1810\n",
            " Global step 75/1810\n",
            " Global step 76/1810\n",
            " Global step 77/1810\n",
            " Global step 78/1810\n",
            " Global step 79/1810\n",
            " Global step 80/1810\n",
            " Global step 81/1810\n",
            " Global step 82/1810\n",
            " Global step 83/1810\n",
            " Global step 84/1810\n",
            " Global step 85/1810\n",
            " Global step 86/1810\n",
            " Global step 87/1810\n",
            " Global step 88/1810\n",
            " Global step 89/1810\n",
            " Global step 90/1810\n",
            " Global step 91/1810\n",
            " Global step 92/1810\n",
            " Global step 93/1810\n",
            " Global step 94/1810\n",
            " Global step 95/1810\n",
            " Global step 96/1810\n",
            " Global step 97/1810\n",
            " Global step 98/1810\n",
            " Global step 99/1810\n",
            " Global step 100/1810\n",
            " Global step 101/1810\n",
            " Global step 102/1810\n",
            " Global step 103/1810\n",
            " Global step 104/1810\n",
            " Global step 105/1810\n",
            " Global step 106/1810\n",
            " Global step 107/1810\n",
            " Global step 108/1810\n",
            " Global step 109/1810\n",
            " Global step 110/1810\n",
            " Global step 111/1810\n",
            " Global step 112/1810\n",
            " Global step 113/1810\n",
            " Global step 114/1810\n",
            " Global step 115/1810\n",
            " Global step 116/1810\n",
            " Global step 117/1810\n",
            " Global step 118/1810\n",
            " Global step 119/1810\n",
            " Global step 120/1810\n",
            " Global step 121/1810\n",
            " Global step 122/1810\n",
            " Global step 123/1810\n",
            " Global step 124/1810\n",
            " Global step 125/1810\n",
            " Global step 126/1810\n",
            " Global step 127/1810\n",
            " Global step 128/1810\n",
            " Global step 129/1810\n",
            " Global step 130/1810\n",
            " Global step 131/1810\n",
            " Global step 132/1810\n",
            " Global step 133/1810\n",
            " Global step 134/1810\n",
            " Global step 135/1810\n",
            " Global step 136/1810\n",
            " Global step 137/1810\n",
            " Global step 138/1810\n",
            " Global step 139/1810\n",
            " Global step 140/1810\n",
            " Global step 141/1810\n",
            " Global step 142/1810\n",
            " Global step 143/1810\n",
            " Global step 144/1810\n",
            " Global step 145/1810\n",
            " Global step 146/1810\n",
            " Global step 147/1810\n",
            " Global step 148/1810\n",
            " Global step 149/1810\n",
            " Global step 150/1810\n",
            " Global step 151/1810\n",
            " Global step 152/1810\n",
            " Global step 153/1810\n",
            " Global step 154/1810\n",
            " Global step 155/1810\n",
            " Global step 156/1810\n",
            " Global step 157/1810\n",
            " Global step 158/1810\n",
            " Global step 159/1810\n",
            " Global step 160/1810\n",
            " Global step 161/1810\n",
            " Global step 162/1810\n",
            " Global step 163/1810\n",
            " Global step 164/1810\n",
            " Global step 165/1810\n",
            " Global step 166/1810\n",
            " Global step 167/1810\n",
            " Global step 168/1810\n",
            " Global step 169/1810\n",
            " Global step 170/1810\n",
            " Global step 171/1810\n",
            " Global step 172/1810\n",
            " Global step 173/1810\n",
            " Global step 174/1810\n",
            " Global step 175/1810\n",
            " Global step 176/1810\n",
            " Global step 177/1810\n",
            " Global step 178/1810\n",
            " Global step 179/1810\n",
            " Global step 180/1810\n",
            " Global step 181/1810\n",
            " Global step 182/1810\n",
            " Global step 183/1810\n",
            " Global step 184/1810\n",
            " Global step 185/1810\n",
            " Global step 186/1810\n",
            " Global step 187/1810\n",
            " Global step 188/1810\n",
            " Global step 189/1810\n",
            " Global step 190/1810\n",
            " Global step 191/1810\n",
            " Global step 192/1810\n",
            " Global step 193/1810\n",
            " Global step 194/1810\n",
            " Global step 195/1810\n",
            " Global step 196/1810\n",
            " Global step 197/1810\n",
            " Global step 198/1810\n",
            " Global step 199/1810\n",
            " Global step 200/1810\n",
            " Global step 201/1810\n",
            " Global step 202/1810\n",
            " Global step 203/1810\n",
            " Global step 204/1810\n",
            " Global step 205/1810\n",
            " Global step 206/1810\n",
            " Global step 207/1810\n",
            " Global step 208/1810\n",
            " Global step 209/1810\n",
            " Global step 210/1810\n",
            " Global step 211/1810\n",
            " Global step 212/1810\n",
            " Global step 213/1810\n",
            " Global step 214/1810\n",
            " Global step 215/1810\n",
            " Global step 216/1810\n",
            " Global step 217/1810\n",
            " Global step 218/1810\n",
            " Global step 219/1810\n",
            " Global step 220/1810\n",
            " Global step 221/1810\n",
            " Global step 222/1810\n",
            " Global step 223/1810\n",
            " Global step 224/1810\n",
            " Global step 225/1810\n",
            " Global step 226/1810\n",
            " Global step 227/1810\n",
            " Global step 228/1810\n",
            " Global step 229/1810\n",
            " Global step 230/1810\n",
            " Global step 231/1810\n",
            " Global step 232/1810\n",
            " Global step 233/1810\n",
            " Global step 234/1810\n",
            " Global step 235/1810\n",
            " Global step 236/1810\n",
            " Global step 237/1810\n",
            " Global step 238/1810\n",
            " Global step 239/1810\n",
            " Global step 240/1810\n",
            " Global step 241/1810\n",
            " Global step 242/1810\n",
            " Global step 243/1810\n",
            " Global step 244/1810\n",
            " Global step 245/1810\n",
            " Global step 246/1810\n",
            " Global step 247/1810\n",
            " Global step 248/1810\n",
            " Global step 249/1810\n",
            " Global step 250/1810\n",
            " Global step 251/1810\n",
            " Global step 252/1810\n",
            " Global step 253/1810\n",
            " Global step 254/1810\n",
            " Global step 255/1810\n",
            " Global step 256/1810\n",
            " Global step 257/1810\n",
            " Global step 258/1810\n",
            " Global step 259/1810\n",
            " Global step 260/1810\n",
            " Global step 261/1810\n",
            " Global step 262/1810\n",
            " Global step 263/1810\n",
            " Global step 264/1810\n",
            " Global step 265/1810\n",
            " Global step 266/1810\n",
            " Global step 267/1810\n",
            " Global step 268/1810\n",
            " Global step 269/1810\n",
            " Global step 270/1810\n",
            " Global step 271/1810\n",
            " Global step 272/1810\n",
            " Global step 273/1810\n",
            " Global step 274/1810\n",
            " Global step 275/1810\n",
            " Global step 276/1810\n",
            " Global step 277/1810\n",
            " Global step 278/1810\n",
            " Global step 279/1810\n",
            " Global step 280/1810\n",
            " Global step 281/1810\n",
            " Global step 282/1810\n",
            " Global step 283/1810\n",
            " Global step 284/1810\n",
            " Global step 285/1810\n",
            " Global step 286/1810\n",
            " Global step 287/1810\n",
            " Global step 288/1810\n",
            " Global step 289/1810\n",
            " Global step 290/1810\n",
            " Global step 291/1810\n",
            " Global step 292/1810\n",
            " Global step 293/1810\n",
            " Global step 294/1810\n",
            " Global step 295/1810\n",
            " Global step 296/1810\n",
            " Global step 297/1810\n",
            " Global step 298/1810\n",
            " Global step 299/1810\n",
            " Global step 300/1810\n",
            " Global step 301/1810\n",
            " Global step 302/1810\n",
            " Global step 303/1810\n",
            " Global step 304/1810\n",
            " Global step 305/1810\n",
            " Global step 306/1810\n",
            " Global step 307/1810\n",
            " Global step 308/1810\n",
            " Global step 309/1810\n",
            " Global step 310/1810\n",
            " Global step 311/1810\n",
            " Global step 312/1810\n",
            " Global step 313/1810\n",
            " Global step 314/1810\n",
            " Global step 315/1810\n",
            " Global step 316/1810\n",
            " Global step 317/1810\n",
            " Global step 318/1810\n",
            " Global step 319/1810\n",
            " Global step 320/1810\n",
            " Global step 321/1810\n",
            " Global step 322/1810\n",
            " Global step 323/1810\n",
            " Global step 324/1810\n",
            " Global step 325/1810\n",
            " Global step 326/1810\n",
            " Global step 327/1810\n",
            " Global step 328/1810\n",
            " Global step 329/1810\n",
            " Global step 330/1810\n",
            " Global step 331/1810\n",
            " Global step 332/1810\n",
            " Global step 333/1810\n",
            " Global step 334/1810\n",
            " Global step 335/1810\n",
            " Global step 336/1810\n",
            " Global step 337/1810\n",
            " Global step 338/1810\n",
            " Global step 339/1810\n",
            " Global step 340/1810\n",
            " Global step 341/1810\n",
            " Global step 342/1810\n",
            " Global step 343/1810\n",
            " Global step 344/1810\n",
            " Global step 345/1810\n",
            " Global step 346/1810\n",
            " Global step 347/1810\n",
            " Global step 348/1810\n",
            " Global step 349/1810\n",
            " Global step 350/1810\n",
            " Global step 351/1810\n",
            " Global step 352/1810\n",
            " Global step 353/1810\n",
            " Global step 354/1810\n",
            " Global step 355/1810\n",
            " Global step 356/1810\n",
            " Global step 357/1810\n",
            " Global step 358/1810\n",
            " Global step 359/1810\n",
            " Global step 360/1810\n",
            " Global step 361/1810\n",
            " Global step 362/1810\n",
            " Global step 363/1810"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiT9QaPn8glH"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkNS7l2o8t4J"
      },
      "source": [
        "optimizer = keras.optimizers.Nadam(lr=0.01)\n",
        "loss_fn = keras.losses.mean_squared_error\n",
        "\n",
        "@tf.function\n",
        "def train(model, n_epochs, batch_size=32, n_readers=5, n_read_threads=5, shuffle_buffer_size=10000,\n",
        "          n_parse_threads=5):\n",
        "  train_set = csv_reader_dataset(train_filepaths, repeat=n_epochs, n_readers=n_reader, n_read_threads=n_read_threads, shuffle_buffer_size=shuffle_buffer_size,\n",
        "                                 n_parse_threads=n_parse_threads, batch_size=batch_size)\n",
        "  for x_batch, y_batch in train_set:\n",
        "    with tf.GradientTape() as tape:\n",
        "      y_pred = model(x_batch)\n",
        "      main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
        "      loss = tf.add_n(\n",
        "          [main_loss] + model.losses\n",
        "      )\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(\n",
        "        zip(gradients, model.trainable_variables)\n",
        "    )\n",
        "  train(model, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jI8nz_BATEuu"
      },
      "source": [
        "13.2 TFRecord 포맷"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uq-OtzFkTJik"
      },
      "source": [
        "with tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\n",
        "  f.write(b\"This is the first record\")\n",
        "  f.write(b\"And this is the second record\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HN3PtEVETTOI",
        "outputId": "e14a136e-c6e6-4a03-9cce-9c5d06f2c81b"
      },
      "source": [
        "filepaths = [\"my_data.tfrecord\"]\n",
        "dataset = tf.data.TFRecordDataset(filepaths)\n",
        "for item in dataset:\n",
        "  print(item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(b'This is the first record', shape=(), dtype=string)\n",
            "tf.Tensor(b'And this is the second record', shape=(), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUfMURycTZjB"
      },
      "source": [
        "13.2.1 압축된 TFRecord 파일"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z02IaF5rUExP"
      },
      "source": [
        "options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
        "with tf.io.TFRecordWriter(\"my_compressed.tfrecord\", options) as f:\n",
        "  f.write(b\"This is the first record\")\n",
        "  f.write(b\"And this is the second record\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaU2Lc_CUOVn",
        "outputId": "635e4411-0f81-4d51-8a9f-09272d140146"
      },
      "source": [
        "dataset = tf.data.TFRecordDataset(\n",
        "    [\"my_compressed.tfrecord\"], compression_type=\"GZIP\"\n",
        ")\n",
        "\n",
        "for item in dataset:\n",
        "  print(item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(b'This is the first record', shape=(), dtype=string)\n",
            "tf.Tensor(b'And this is the second record', shape=(), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Lw8mIF-UoTi"
      },
      "source": [
        "13.2.2 프로토콜 버퍼 개요"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9RcIcnHUuOv",
        "outputId": "220d49a2-e03c-41ae-b129-d53653984e63"
      },
      "source": [
        "%%writefile person.proto\n",
        "\n",
        "syntax = \"proto3\"\n",
        "message Person {\n",
        "    string name = 1;\n",
        "    int32 id = 2;\n",
        "    repeated string email = 3;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting person.proto\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooXxiDpCVvd0",
        "outputId": "12a340d3-7e4a-4f01-e035-5d60d95a0032"
      },
      "source": [
        "!protoc person.proto --python_out=. --descriptor_set_out=person.desc --include_imports"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "person.proto:3:1: Expected \";\".\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3YSzA3tV3Oa",
        "outputId": "77408a95-ef65-46f2-b895-467a0932e86c"
      },
      "source": [
        "!ls person*"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "person.desc  person_pb2.py  person.proto\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcL9YMVsU4Ag",
        "outputId": "dd0bd699-6256-4b38-e27a-95d0ba0535a9"
      },
      "source": [
        "from person_pb2 import Person #생성된 클래스 임포트\n",
        "person = Person(name=\"Al\", id=123, email=[\"a@b.com\"]) #객체 생성\n",
        "print(person)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "name: \"Al\"\n",
            "id: 123\n",
            "email: \"a@b.com\"\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "27XRkQN2VEer",
        "outputId": "718df999-a369-462f-da4a-2fe733b44ec0"
      },
      "source": [
        "person.name"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Al'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZHqfGu_aWFey",
        "outputId": "9ab217d6-c45c-4eb3-a563-fc95a3a6ca0d"
      },
      "source": [
        "person.email[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'a@b.com'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuJbktpeWGk6"
      },
      "source": [
        "person.email.append(\"c@d.com\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQ_XIm6VWI3n"
      },
      "source": [
        "s = person.SerializeToString()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9m2INAKWKtq",
        "outputId": "a1d63dcc-7973-47c3-ce97-308825d15269"
      },
      "source": [
        "s\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'\\n\\x05Alice\\x10{\\x1a\\x07a@b.com\\x1a\\x07c@d.com\\x1a\\x07c@d.com'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwlB2Lk1WL00"
      },
      "source": [
        "person2 = Person()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJenFJ4tWOMq",
        "outputId": "d7e03b29-5880-49a3-94f7-41da1ae7f176"
      },
      "source": [
        "person2.ParseFromString(s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxPSQoBHWPto",
        "outputId": "c448fa91-b453-4679-f672-ceb2337fa25e"
      },
      "source": [
        "person == person2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVZAxN6fWRji"
      },
      "source": [
        "person.name = \"Alice\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz643-OpWVfJ"
      },
      "source": [
        "13.2.3 텐서플로 프로토콜 버퍼"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMO2KQnZaRWR"
      },
      "source": [
        "# The following functions can be used to convert a value to a type compatible\n",
        "# with tf.Example.\n",
        "\n",
        "def _bytes_feature(value):\n",
        "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "def _float_feature(value):\n",
        "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
        "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
        "\n",
        "def _int64_feature(value):\n",
        "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
        "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1rWymfuWdQX",
        "outputId": "d9f92df1-525c-460a-ca39-76f084c5ef5f"
      },
      "source": [
        "%%writefile Feature.proto\n",
        "\n",
        "syntax = \"proto3\";\n",
        "\n",
        "message BytesList { repeated bytes value = 1; }\n",
        "message FloatList { repeated float value = 1 [packed = true]; }\n",
        "message Int64List { repeated int64 value = 1 [packed = true]; }\n",
        "message Feature {\n",
        "    oneof kind {\n",
        "        BytesList bytes_list = 1;\n",
        "        FloatList float_list = 2;\n",
        "        Int64List int64_list = 3;\n",
        "    }\n",
        "};\n",
        "message Features { map<string, Feature> feature = 1; };\n",
        "message Example { Featrues features };"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting Feature.proto\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wmkn9k6-W8S3"
      },
      "source": [
        "#from tensorflow.train import BytesList, FloatList, Int64List\n",
        "#from tensorflow.train import Feature, Features, Example\n",
        "\n",
        "BytesList = tf.train.BytesList\n",
        "FloatList = tf.train.FloatList\n",
        "Int64List = tf.train.Int64List\n",
        "Feature = tf.train.Feature\n",
        "Features = tf.train.Features\n",
        "Example = tf.train.Example\n",
        "\n",
        "person_example = Example(\n",
        "    features = Features(\n",
        "        feature={\n",
        "            \"name\": Feature(bytes_list=BytesList(value=[b\"Alice\"])),\n",
        "            \"id\": Feature(int64_list=Int64List(value=[123])),\n",
        "            \"emails\": Feature(bytes_list=BytesList(value=[b\"a@b.com\", b\"c@d.com\"]))\n",
        "        }\n",
        "    )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "876YFMTLhi8p"
      },
      "source": [
        "with tf.io.TFRecordWriter(\"my_contacts.tfrecord\") as f:\n",
        "  f.write(person_example.SerializeToString())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARlqA4DlicTe"
      },
      "source": [
        "13.2.3 Example 프로토콜 버퍼를 읽고 파싱하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKpfr8OzijJH"
      },
      "source": [
        "feature_description = {\n",
        "    \"name\": tf.io.FixedLenFeature(\n",
        "        [], tf.string, default_value=\"\"\n",
        "    ),\n",
        "    \"id\": tf.io.FixedLenFeature(\n",
        "        [], tf.int64, default_value=0\n",
        "    ),\n",
        "    \"emails\": tf.io.VarLenFeature(tf.string)\n",
        "}\n",
        "\n",
        "for serialized_example in tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]):\n",
        "  parsed_example = tf.io.parse_single_example(serialized_example, feature_description)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piGyhTLAi_5k",
        "outputId": "88d3ed13-b4a3-407a-af20-31cfc3d65419"
      },
      "source": [
        "tf.sparse.to_dense(\n",
        "    parsed_example[\"emails\"], default_value=b\"\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'a@b.com', b'c@d.com'], dtype=object)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9J1lD1djHud",
        "outputId": "91af1067-03b6-40b2-dd51-cc7275868c2a"
      },
      "source": [
        "parsed_example[\"emails\"].values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'a@b.com', b'c@d.com'], dtype=object)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGrFbaa_jK9o"
      },
      "source": [
        "dataset = tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]).batch(10)\n",
        "for serialized_example in dataset:\n",
        "  parsed_examples = tf.io.parse_example(serialized_example, feature_description)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWYXRqVtjVJC"
      },
      "source": [
        "13.2.4 SequenceExample 프로토콜 버퍼를 사용해 리스트의 리스트 다루기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jykryYcXjZJw",
        "outputId": "5b927daa-b590-414b-be67-b6a7cd9d206d"
      },
      "source": [
        "%%writefile Sequence.proto\n",
        "\n",
        "syntax = \"proto3\";\n",
        "\n",
        "message FeatureList { repeated Feature feature = 1; };\n",
        "message FeatureLists { map<string, FeatureList> feature_list = 1; };\n",
        "message SequenceExample {\n",
        "    Feature context = 1;\n",
        "    FeatureLists feature_lists = 2;\n",
        "};"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting Sequence.proto\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-c_Swvo5leVK",
        "outputId": "24a7c6e7-1801-42ed-e732-991ab5d8a793"
      },
      "source": [
        "parsed_examples"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'emails': <tensorflow.python.framework.sparse_tensor.SparseTensor at 0x7fce2757f4e0>,\n",
              " 'id': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([123])>,\n",
              " 'name': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Alice'], dtype=object)>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POQy4BLwlLHq"
      },
      "source": [
        "serialized_sequence_example = sequence_example.SerializeToString()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3JkUcOEkb3l"
      },
      "source": [
        "\n",
        "#from tensorflow.train import FeatureList, FeatureLists, SequenceExample\n",
        "FeatureList = tf.train.FeatureList\n",
        "FeatureLists = tf.train.FeatureLists\n",
        "SequenceExample = tf.train.SequenceExample\n",
        "\n",
        "context_feature_descriptions = {\n",
        "    \"author_id\": tf.io.FixedLenFeature(\n",
        "        [], tf.int64, default_value = 0\n",
        "    ),\n",
        "    \"title\": tf.io.VarLenFeature(tf.string),\n",
        "    \"pub_date\": tf.io.FixedLenFeature(\n",
        "        [3], tf.int64, default_value=[0,0,0]\n",
        "    )\n",
        "}\n",
        "\n",
        "sequence_feature_descriptions = {\n",
        "    \"content\": tf.io.VarLenFeature(tf.string),\n",
        "    \"comments\": tf.io.VarLenFeature(tf.string)\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUOjnJDbj6wy"
      },
      "source": [
        "parsed_context, parsed_feature_lists = tf.io.parse_single_sequence_example(\n",
        "    serialized_sequence_example, context_feature_descriptions, sequence_feature_descriptions\n",
        ")\n",
        "\n",
        "parsed_content = tf.RaggedTensor.from_sparse(parsed_feature_lists[\"content\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uo6cR6P0kMW1"
      },
      "source": [
        "13.3 입력 특성 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luSOBwArnF0o"
      },
      "source": [
        "means = np.mean(x_train, axis=0, keepdims=True)\n",
        "stds = np.std(x_train, axis=0, keepdims=True)\n",
        "eps = keras.backend.epsilon()\n",
        "model = keras.models.Sequential(\n",
        "    [\n",
        "     keras.layers.Lambda(lambda inputs: (inputs - means) / (stds + eps)),\n",
        "     #[...] #다른 층\n",
        "    ]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyMceEDPoE93"
      },
      "source": [
        "keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwX_finVoKPf"
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "907phQ7yoPzj"
      },
      "source": [
        "class Standardzation(keras.layers.Layer):\n",
        "  def adapt(self, data_sample):\n",
        "    self.means_ = np.mean(data_sample, axis=0, keepdims=True)\n",
        "    self.stds_ = np.std(data_sample, axis=0, keepdims=True)\n",
        "  def call(self, inputs):\n",
        "    return (inputs - self.means_) / (self.stds_ + keras.backend.epsilon())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpwzllVuqD0R"
      },
      "source": [
        "std_layer = Standardzation()\n",
        "#std_layer.adapt(data_sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbEHH3vprDLt"
      },
      "source": [
        "model = keras.Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_1yB0OJrP1f"
      },
      "source": [
        "model.add(std_layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jt8NwJ0crRJN"
      },
      "source": [
        "#모델 구성\n",
        "#model.compile([...])\n",
        "#model.fit([...])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmPPVqM3rVJG"
      },
      "source": [
        "13.3.1 원-핫 벡터를 사용해 범주형 특성 인코딩하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJItrkF0rapL"
      },
      "source": [
        "vocab = [\"<1H OCEAN\", \"INLAND\", \"NEAR OCEAN\", \"NEAR BAY\", \"ISLAND\"]\n",
        "indices = tf.range(\n",
        "    len(vocab), dtype=tf.int64\n",
        ")\n",
        "table_init = tf.lookup.KeyValueTensorInitializer(vocab, indices)\n",
        "num_oov_buckets = 2\n",
        "table = tf.lookup.StaticVocabularyTable(table_init, num_oov_buckets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6mbSwvDrt7A",
        "outputId": "480eacc8-632b-402e-ae4d-833a1fdd7801"
      },
      "source": [
        "categories = tf.constant(\n",
        "    [\"NEAR BAY\", \"DESERT\", \"INLAND\", \"INLAND\"]\n",
        ")\n",
        "cat_indices = table.lookup(categories)\n",
        "cat_indices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4,), dtype=int64, numpy=array([3, 5, 1, 1])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDNDwvmyr34D",
        "outputId": "b5d00ba2-9678-4ce3-e80b-397a1c1ea958"
      },
      "source": [
        "cat_one_hot = tf.one_hot(cat_indices, depth=len(vocab) + num_oov_buckets)\n",
        "cat_one_hot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4, 7), dtype=float32, numpy=\n",
              "array([[0., 0., 0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 1., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12x-XyDmr9m1"
      },
      "source": [
        "13.3.2 임베딩을 사용해 범주형 특성 인코딩하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVbFv3lKsAvy"
      },
      "source": [
        "embedding_dim = 2\n",
        "embed_init = tf.random.uniform(\n",
        "    [\n",
        "     len(vocab) + num_oov_buckets, embedding_dim\n",
        "    ]\n",
        ")\n",
        "embedding_metrix = tf.Variable(embed_init)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d1dyg0BsLeU",
        "outputId": "b0fc151a-b326-4920-830c-19ad6e14f91b"
      },
      "source": [
        "embedding_metrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'Variable:0' shape=(7, 2) dtype=float32, numpy=\n",
              "array([[0.6645621 , 0.44100678],\n",
              "       [0.3528825 , 0.46448255],\n",
              "       [0.03366041, 0.68467236],\n",
              "       [0.74011743, 0.8724445 ],\n",
              "       [0.22632635, 0.22319686],\n",
              "       [0.3103881 , 0.7223358 ],\n",
              "       [0.13318717, 0.5480639 ]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhGxyCa4sM8J",
        "outputId": "7b2be6f8-10d9-48d9-e597-fd098413c1c3"
      },
      "source": [
        "categories = tf.constant(\n",
        "    [\"NEAR BAY\", \"DESERT\", \"INLAND\", \"INLAND\"]\n",
        ")\n",
        "cat_indices = table.lookup(categories)\n",
        "cat_indices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4,), dtype=int64, numpy=array([3, 5, 1, 1])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcIyPTrmsVuH",
        "outputId": "3429361e-555b-45c5-bce8-448e63c04d84"
      },
      "source": [
        "tf.nn.embedding_lookup(embedding_metrix, cat_indices)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4, 2), dtype=float32, numpy=\n",
              "array([[0.74011743, 0.8724445 ],\n",
              "       [0.3103881 , 0.7223358 ],\n",
              "       [0.3528825 , 0.46448255],\n",
              "       [0.3528825 , 0.46448255]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUIfnbxOsZbb",
        "outputId": "a17cab9e-eaca-446d-f963-9c8218f835b8"
      },
      "source": [
        "embedding = keras.layers.Embedding(\n",
        "    input_dim=len(vocab) + num_oov_buckets, output_dim=embedding_dim\n",
        ")\n",
        "\n",
        "embedding(cat_indices)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4, 2), dtype=float32, numpy=\n",
              "array([[ 0.04467482,  0.02493341],\n",
              "       [-0.02383961,  0.01973433],\n",
              "       [ 0.04309944, -0.0247813 ],\n",
              "       [ 0.04309944, -0.0247813 ]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 220
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty4O9mqmsiwY"
      },
      "source": [
        "regular_inputs = keras.layers.Input(shape=[8])\n",
        "categories = keras.layers.Input(shape=[], dtype=tf.string)\n",
        "\n",
        "cat_indices = keras.layers.Lambda( lambda cats: table.lookup(cats) )(categories)\n",
        "cat_embed = keras.layers.Embedding(input_dim=6, output_dim=2)(cat_indices)\n",
        "encoded_inputs = keras.layers.concatenate(\n",
        "    [regular_inputs, cat_embed]\n",
        ")\n",
        "outputs = keras.layers.Dense(1)(encoded_inputs)\n",
        "model = keras.models.Model(inputs=[regular_inputs, categories], outputs=[outputs])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRArQaOKtGlM"
      },
      "source": [
        "13.3.3 케라스 전처리 층"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGxKseKftJnl"
      },
      "source": [
        "#normalization = keras.layers.Normalization()\n",
        "#discretization = keras.layers.Discretization([...])\n",
        "##pipeline = keras.layers.PreprocessingStage(\n",
        "#    [normalization, discretization]\n",
        "#)\n",
        "#pipeline.adapt(data_sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkGCqi1ateeJ"
      },
      "source": [
        "13.4 TF 변환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZEouNURt3Jc"
      },
      "source": [
        "try:\n",
        "  import tensorflow_transform as tft\n",
        "\n",
        "  def preprocess(inputs):\n",
        "    median_age = inputs[\"housing_median_age\"]\n",
        "    ocean_proximity = inputs[\"ocean_proximity\"]\n",
        "    standardized_age = tft.scale_to_z_score(median_age)\n",
        "    ocean_proximity_id = tft.compute_and_apply_vocabulary(ocean_proximity)\n",
        "    return {\n",
        "        \"standardized_median_age\": standardized_age,\n",
        "        \"ocean_proximity_id\": ocean_proximity_id\n",
        "    }\n",
        "except ImportError:\n",
        "  print(\"TF transform is not installed try running : pip install -U tensorflow-transform\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iH6YKZHbuSnJ",
        "outputId": "6844f1b8-fab7-411c-c8f0-a48b508772d4"
      },
      "source": [
        "pip install tensorflow-transform"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-transform\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4e/37/5a8224e6462c1d0b9e88e57191145bcb07d9928d93279f7df727a80d24c7/tensorflow_transform-0.25.0-py3-none-any.whl (378kB)\n",
            "\r\u001b[K     |▉                               | 10kB 14.4MB/s eta 0:00:01\r\u001b[K     |█▊                              | 20kB 18.6MB/s eta 0:00:01\r\u001b[K     |██▋                             | 30kB 9.8MB/s eta 0:00:01\r\u001b[K     |███▌                            | 40kB 8.1MB/s eta 0:00:01\r\u001b[K     |████▎                           | 51kB 4.1MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 61kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 71kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████                         | 81kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 92kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 102kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 112kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 122kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 133kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████                    | 143kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 153kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 163kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 174kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 184kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 194kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 204kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 215kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 225kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 235kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 245kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 256kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 266kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 276kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 286kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 296kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 307kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 317kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 327kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 337kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 348kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 358kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 368kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 378kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 389kB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-metadata<0.26,>=0.25 in /usr/local/lib/python3.6/dist-packages (from tensorflow-transform) (0.25.0)\n",
            "Requirement already satisfied: six<2,>=1.12 in /usr/local/lib/python3.6/dist-packages (from tensorflow-transform) (1.15.0)\n",
            "Collecting apache-beam[gcp]<3,>=2.25\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/3f/93816e989e8e59b337f22927778494a99b2a3e78a3b6a9e34d043c6fab4e/apache_beam-2.25.0-cp36-cp36m-manylinux2010_x86_64.whl (8.7MB)\n",
            "\u001b[K     |████████████████████████████████| 8.7MB 18.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pydot<2,>=1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-transform) (1.3.0)\n",
            "Requirement already satisfied: tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-transform) (2.3.0)\n",
            "Collecting tfx-bsl<0.26,>=0.25\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/6e/79c6fb6b212164391fcc562f42cf11217b7e03aa20fc9aa29083ede94190/tfx_bsl-0.25.0-cp36-cp36m-manylinux2010_x86_64.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 39.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py<0.11,>=0.9 in /usr/local/lib/python3.6/dist-packages (from tensorflow-transform) (0.10.0)\n",
            "Requirement already satisfied: numpy<2,>=1.16 in /usr/local/lib/python3.6/dist-packages (from tensorflow-transform) (1.18.5)\n",
            "Requirement already satisfied: protobuf<4,>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-transform) (3.12.4)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata<0.26,>=0.25->tensorflow-transform) (1.52.0)\n",
            "Collecting future<1.0.0,>=0.18.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 34.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio<2,>=1.29.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.25->tensorflow-transform) (1.33.2)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.25->tensorflow-transform) (2018.9)\n",
            "Collecting avro-python3!=1.9.2,<1.10.0,>=1.8.1; python_version >= \"3.0\"\n",
            "  Downloading https://files.pythonhosted.org/packages/5a/80/acd1455bea0a9fcdc60a748a97dcbb3ff624726fb90987a0fc1c19e7a5a5/avro-python3-1.9.2.1.tar.gz\n",
            "Collecting mock<3.0.0,>=1.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/35/f187bdf23be87092bd0f1200d43d23076cee4d0dec109f195173fd3ebc79/mock-2.0.0-py2.py3-none-any.whl (56kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.25->tensorflow-transform) (3.11.1)\n",
            "Collecting pyarrow<0.18.0,>=0.15.1; python_version >= \"3.0\" or platform_system != \"Windows\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/3f/6cac1714fff444664603f92cb9fbe91c7ae25375880158b9e9691c4584c8/pyarrow-0.17.1-cp36-cp36m-manylinux2014_x86_64.whl (63.8MB)\n",
            "\u001b[K     |████████████████████████████████| 63.8MB 66kB/s \n",
            "\u001b[?25hRequirement already satisfied: httplib2<0.18.0,>=0.8 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.25->tensorflow-transform) (0.17.4)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.25->tensorflow-transform) (1.7)\n",
            "Collecting fastavro<2,>=0.21.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/a9/473ef678c8862d74c63e11d14afbdbeabe67f92fedd82405de5337d7e6de/fastavro-1.2.0-cp36-cp36m-manylinux2014_x86_64.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 38.1MB/s \n",
            "\u001b[?25hCollecting hdfs<3.0.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/82/39/2c0879b1bcfd1f6ad078eb210d09dbce21072386a3997074ee91e60ddc5a/hdfs-2.5.8.tar.gz (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauth2client<5,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.25->tensorflow-transform) (4.1.3)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.25->tensorflow-transform) (2.8.1)\n",
            "Requirement already satisfied: typing-extensions<3.8.0,>=3.7.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.25->tensorflow-transform) (3.7.4.3)\n",
            "Collecting dill<0.3.2,>=0.3.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/11/345f3173809cea7f1a193bfbf02403fff250a3360e0e118a1630985e547d/dill-0.3.1.1.tar.gz (151kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 43.2MB/s \n",
            "\u001b[?25hCollecting requests<3.0.0,>=2.24.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/39/fc/f91eac5a39a65f75a7adb58eac7fa78871ea9872283fb9c44e6545998134/requests-2.25.0-py2.py3-none-any.whl (61kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: cachetools<5,>=3.1.0; extra == \"gcp\" in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.25->tensorflow-transform) (4.1.1)\n",
            "Collecting google-cloud-language<2,>=1.3.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/b8/965a97ba60287910d342623da1da615254bded3e0965728cf7fc6339b7c8/google_cloud_language-1.3.0-py2.py3-none-any.whl (83kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 9.9MB/s \n",
            "\u001b[?25hCollecting google-auth<2,>=1.18.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/60/81e68e70eea91ef05bb00bcdac243d67b61f826c65aaca6961de622dffd7/google_auth-1.23.0-py2.py3-none-any.whl (114kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 35.7MB/s \n",
            "\u001b[?25hCollecting google-cloud-spanner<2,>=1.13.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/16/c9262ca40f3a278f38df9d21dece1ae01ee24f8ed29937bd1f066f908f9f/google_cloud_spanner-1.19.1-py2.py3-none-any.whl (255kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 39.9MB/s \n",
            "\u001b[?25hCollecting google-apitools<0.5.32,>=0.5.31; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/da/aefc4cf4c168b5d875344cd9dddc77e3a2d11986b630251af5ce47dd2843/google-apitools-0.5.31.tar.gz (173kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 45.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-core<2,>=0.28.1; extra == \"gcp\" in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.25->tensorflow-transform) (1.0.3)\n",
            "Collecting grpcio-gcp<1,>=0.2.2; extra == \"gcp\"\n",
            "  Downloading https://files.pythonhosted.org/packages/ba/83/1f1095815be0de19102df41e250ebbd7dae97d7d14e22c18da07ed5ed9d4/grpcio_gcp-0.2.2-py2.py3-none-any.whl\n",
            "Collecting google-cloud-vision<2,>=0.38.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0d/7f/e10d602c2dc3f749f1b78377a3357790f1da71b28e7da9e5bc20b3a9bd40/google_cloud_vision-1.0.0-py2.py3-none-any.whl (435kB)\n",
            "\u001b[K     |████████████████████████████████| 440kB 43.3MB/s \n",
            "\u001b[?25hCollecting google-cloud-dlp<2,>=0.12.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/c3/5b73c15f59207b20df288573c2ea203c7b126df8330add380d8b50bc0d5c/google_cloud_dlp-1.0.0-py2.py3-none-any.whl (169kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 48.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-datastore<2,>=1.7.1; extra == \"gcp\" in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.25->tensorflow-transform) (1.8.0)\n",
            "Collecting google-cloud-pubsub<2,>=0.39.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/b3/dd83eca4cd1019d592e82595ea45d53f11e39db4ee99daa66ceb8a1b2d89/google_cloud_pubsub-1.7.0-py2.py3-none-any.whl (144kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 42.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-bigquery<2,>=1.6.0; extra == \"gcp\" in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.25->tensorflow-transform) (1.21.0)\n",
            "Collecting google-cloud-videointelligence<2,>=1.8.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/29/8d06211102c87768dc34943d9c92abd8b67491ffedd6d09b56305b1ab255/google_cloud_videointelligence-1.16.1-py2.py3-none-any.whl (183kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 44.3MB/s \n",
            "\u001b[?25hCollecting google-cloud-bigtable<2,>=0.31.1; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/f5/0ab49a0ff634168da8960629d4eed391d4debe176f382636d1993c4145bb/google_cloud_bigtable-1.6.0-py2.py3-none-any.whl (267kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 43.6MB/s \n",
            "\u001b[?25hCollecting google-cloud-build<3,>=2.0.0; python_version >= \"3.6\" and extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/04/b566b23f3fdea72326efce6aef09e523b6f8ef3058959672f673c41ffaca/google_cloud_build-2.0.0-py2.py3-none-any.whl (67kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.6/dist-packages (from pydot<2,>=1.2->tensorflow-transform) (2.4.7)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow-transform) (3.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow-transform) (0.2.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow-transform) (1.6.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow-transform) (1.12.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow-transform) (2.3.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow-transform) (1.1.2)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow-transform) (2.3.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow-transform) (1.4.1)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow-transform) (2.10.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow-transform) (0.3.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow-transform) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow-transform) (0.35.1)\n",
            "Collecting tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,<3,>=1.15\n",
            "  Downloading https://files.pythonhosted.org/packages/72/0d/0f0822b418fd51795a4768f35bb17619af51312ca9f5762c80bea34bd7ae/tensorflow_serving_api-2.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pandas<2,>=1.0 in /usr/local/lib/python3.6/dist-packages (from tfx-bsl<0.26,>=0.25->tensorflow-transform) (1.1.4)\n",
            "Requirement already satisfied: google-api-python-client<2,>=1.7.11 in /usr/local/lib/python3.6/dist-packages (from tfx-bsl<0.26,>=0.25->tensorflow-transform) (1.7.12)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf<4,>=3.9.2->tensorflow-transform) (50.3.2)\n",
            "Collecting pbr>=0.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/48/69046506f6ac61c1eaa9a0d42d22d54673b69e176d30ca98e3f61513e980/pbr-5.5.1-py2.py3-none-any.whl (106kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 43.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt in /usr/local/lib/python3.6/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.25->tensorflow-transform) (0.6.2)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client<5,>=2.0.1->apache-beam[gcp]<3,>=2.25->tensorflow-transform) (0.4.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client<5,>=2.0.1->apache-beam[gcp]<3,>=2.25->tensorflow-transform) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client<5,>=2.0.1->apache-beam[gcp]<3,>=2.25->tensorflow-transform) (0.2.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.25->tensorflow-transform) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.25->tensorflow-transform) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.25->tensorflow-transform) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.25->tensorflow-transform) (2.10)\n",
            "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-language<2,>=1.3.0; extra == \"gcp\"->apache-beam[gcp]<3,>=2.25->tensorflow-transform) (1.16.0)\n",
            "Collecting grpc-google-iam-v1<0.13dev,>=0.12.3\n",
            "  Downloading https://files.pythonhosted.org/packages/65/19/2060c8faa325fddc09aa67af98ffcb6813f39a0ad805679fa64815362b3a/grpc-google-iam-v1-0.12.3.tar.gz\n",
            "Collecting fasteners>=0.14\n",
            "  Downloading https://files.pythonhosted.org/packages/18/bd/55eb2d6397b9c0e263af9d091ebdb756b15756029b3cededf6461481bc63/fasteners-0.15-py2.py3-none-any.whl\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery<2,>=1.6.0; extra == \"gcp\"->apache-beam[gcp]<3,>=2.25->tensorflow-transform) (0.4.1)\n",
            "Collecting proto-plus>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/0d/dd8e8c979eb843b873aeaadcfefd5383da7e2fc5593456a32264cae1cc5c/proto-plus-1.11.0.tar.gz (44kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.4MB/s \n",
            "\u001b[?25hCollecting libcst>=0.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/06/fae14090e8a98e5577aa9b7aac3090d9aba26e9057fb03baa65e38f81183/libcst-0.3.14-py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 36.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow-transform) (1.7.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow-transform) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow-transform) (3.3.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow-transform) (0.4.2)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client<2,>=1.7.11->tfx-bsl<0.26,>=0.25->tensorflow-transform) (3.0.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client<2,>=1.7.11->tfx-bsl<0.26,>=0.25->tensorflow-transform) (0.0.4)\n",
            "Collecting monotonic>=0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/ac/aa/063eca6a416f397bd99552c534c6d11d57f58f2e94c14780f3bbf818c4cf/monotonic-1.5-py2.py3-none-any.whl\n",
            "Collecting pyyaml>=5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 45.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses>=0.6.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from libcst>=0.2.5->google-cloud-build<3,>=2.0.0; python_version >= \"3.6\" and extra == \"gcp\"->apache-beam[gcp]<3,>=2.25->tensorflow-transform) (0.8)\n",
            "Collecting typing-inspect>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/42/1c/66402db44184904a2f14722d317a4da0b5c8c78acfc3faf74362566635c5/typing_inspect-0.6.0-py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow-transform) (2.0.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow-transform) (1.3.0)\n",
            "Collecting mypy-extensions>=0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/eb/975c7c080f3223a5cdaff09612f3a5221e4ba534f7039db34c35d95fa6a5/mypy_extensions-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow-transform) (3.4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2->tensorflow-transform) (3.1.0)\n",
            "Building wheels for collected packages: future, avro-python3, hdfs, dill, google-apitools, grpc-google-iam-v1, proto-plus, pyyaml\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp36-none-any.whl size=491057 sha256=866a197ed66e19f78162513cc4af437547a138b86751b3e52d8b1f6d040c8060\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for avro-python3: filename=avro_python3-1.9.2.1-cp36-none-any.whl size=43516 sha256=33232abd8e74c2d71e8c33d2bd34c942613a94509333d33feb525f0e5eede476\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/d3/be/86620c9dd3fca68986c33b9c616510289fc0abb81ec9aa70bd\n",
            "  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdfs: filename=hdfs-2.5.8-cp36-none-any.whl size=33213 sha256=d2aee5ce44b81fb624f33b2180751e35b34c8f7839e5f4ce37bf6e991986dd7d\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/a7/05/23e3699975fc20f8a30e00ac1e515ab8c61168e982abe4ce70\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-cp36-none-any.whl size=78532 sha256=29048568d453a43793778301341035191fb28c7e1eb451dbb1b8cc4cfd4d399b\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/b1/91/f02e76c732915c4015ab4010f3015469866c1eb9b14058d8e7\n",
            "  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-apitools: filename=google_apitools-0.5.31-cp36-none-any.whl size=131042 sha256=33d52ee0d8303f49842d3d6974875c7f2a90c96309c22dfd5918f66bf372c6fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/43/31/09a9dad88d3aec6fed2d63bd35dfc532fca372e2edec5af5bf\n",
            "  Building wheel for grpc-google-iam-v1 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grpc-google-iam-v1: filename=grpc_google_iam_v1-0.12.3-cp36-none-any.whl size=18500 sha256=92f88df3013d1a5f2b0de27ab5e22fcb6b6c020a3209ff25d701b791e4590797\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/3a/83/77a1e18e1a8757186df834b86ce6800120ac9c79cd8ca4091b\n",
            "  Building wheel for proto-plus (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for proto-plus: filename=proto_plus-1.11.0-cp36-none-any.whl size=41571 sha256=bb022c2a50c9e77a175326b43fad3d529d0204a5a267f518da9570acbedf0477\n",
            "  Stored in directory: /root/.cache/pip/wheels/78/e3/6b/a14506581b1cde1ac1743f2939dcc06fc06a5af2aa224a334e\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44619 sha256=73e38be80578abbd806627b6e6f93a150d502c66ab620009870b720c7fa24d0b\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "Successfully built future avro-python3 hdfs dill google-apitools grpc-google-iam-v1 proto-plus pyyaml\n",
            "\u001b[31mERROR: multiprocess 0.70.11.1 has requirement dill>=0.3.3, but you'll have dill 0.3.1.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.17.2, but you'll have google-auth 1.23.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.25.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-cloud-spanner 1.19.1 has requirement google-cloud-core<2.0dev,>=1.4.1, but you'll have google-cloud-core 1.0.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-cloud-bigtable 1.6.0 has requirement google-cloud-core<2.0dev,>=1.4.1, but you'll have google-cloud-core 1.0.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-cloud-build 2.0.0 has requirement google-api-core[grpc]<2.0.0dev,>=1.22.0, but you'll have google-api-core 1.16.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: future, avro-python3, pbr, mock, pyarrow, fastavro, requests, hdfs, dill, google-cloud-language, google-auth, grpc-google-iam-v1, google-cloud-spanner, monotonic, fasteners, google-apitools, grpcio-gcp, google-cloud-vision, google-cloud-dlp, google-cloud-pubsub, google-cloud-videointelligence, google-cloud-bigtable, proto-plus, pyyaml, mypy-extensions, typing-inspect, libcst, google-cloud-build, apache-beam, tensorflow-serving-api, tfx-bsl, tensorflow-transform\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Found existing installation: dill 0.3.3\n",
            "    Uninstalling dill-0.3.3:\n",
            "      Successfully uninstalled dill-0.3.3\n",
            "  Found existing installation: google-cloud-language 1.2.0\n",
            "    Uninstalling google-cloud-language-1.2.0:\n",
            "      Successfully uninstalled google-cloud-language-1.2.0\n",
            "  Found existing installation: google-auth 1.17.2\n",
            "    Uninstalling google-auth-1.17.2:\n",
            "      Successfully uninstalled google-auth-1.17.2\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed apache-beam-2.25.0 avro-python3-1.9.2.1 dill-0.3.1.1 fastavro-1.2.0 fasteners-0.15 future-0.18.2 google-apitools-0.5.31 google-auth-1.23.0 google-cloud-bigtable-1.6.0 google-cloud-build-2.0.0 google-cloud-dlp-1.0.0 google-cloud-language-1.3.0 google-cloud-pubsub-1.7.0 google-cloud-spanner-1.19.1 google-cloud-videointelligence-1.16.1 google-cloud-vision-1.0.0 grpc-google-iam-v1-0.12.3 grpcio-gcp-0.2.2 hdfs-2.5.8 libcst-0.3.14 mock-2.0.0 monotonic-1.5 mypy-extensions-0.4.3 pbr-5.5.1 proto-plus-1.11.0 pyarrow-0.17.1 pyyaml-5.3.1 requests-2.25.0 tensorflow-serving-api-2.3.0 tensorflow-transform-0.25.0 tfx-bsl-0.25.0 typing-inspect-0.6.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "pyarrow",
                  "requests",
                  "yaml"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVCFjWsUutuj"
      },
      "source": [
        "13.5 텐서플로 데이터셋(TFDS) 프로젝트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211,
          "referenced_widgets": [
            "97db3f173b9f4bcb84a3c7745eb2c05f",
            "11e23f1329d646bca04a17052b958480",
            "ac1292c4ed5745a480bc6bdbde16cdae",
            "4d98c06d7300431ea903cf9303f757ec",
            "f50ed3baa45b413abe9c7e716a926ad2",
            "a858cf5fe291421488eb028914ad48a1",
            "87a80960e4114945a9be76359dfdcbe3",
            "6050d2c8611b427691cb4f952ed5dc52"
          ]
        },
        "id": "YFWzrmUVvNcY",
        "outputId": "cf3a13da-fd23-4152-f364-f2b85c15a382"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "dataset = tfds.load(name=\"mnist\")\n",
        "mnist_train, mnist_test = dataset[\"train\"], dataset[\"test\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset mnist/3.0.1 (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /root/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\n",
            "local data directory. If you'd instead prefer to read directly from our public\n",
            "GCS bucket (recommended if you're running on GCP), you can instead pass\n",
            "`try_gcs=True` to `tfds.load` or set `data_dir=gs://tfds-data/datasets`.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "97db3f173b9f4bcb84a3c7745eb2c05f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Dl Completed...', max=4.0, style=ProgressStyle(descriptio…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1mDataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2R3QGJEvXm_"
      },
      "source": [
        "mnist_train = mnist_train.shuffle(10000).batch(32)\n",
        "mnist_train = mnist_train.map(lambda items: (\n",
        "    item[\"image\"], items[\"label\"]\n",
        "))\n",
        "mnist_train = mnist_train.prefetch(1)\n",
        "#for item in mnist_train:\n",
        "#  images = item[\"image\"]\n",
        "#  labels = item[\"label\"]\n",
        "#  for idx in range(5):\n",
        "#    plt.subplot(1, 5, idx + 1)\n",
        "#    image = images[idx, ..., 0]\n",
        "#    label = labels[idx].numpy()\n",
        "    #plt.imshow(image, cmap=\"binary\")\n",
        "#    plt.title(label)\n",
        "##    plt.axis(\"off\")\n",
        "#  break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 826
        },
        "id": "8dAECAutvvTB",
        "outputId": "ff3d918f-62fd-4547-b5db-53caee77e415"
      },
      "source": [
        "dataset = tfds.load(name=\"mnist\", batch_size=32, as_supervised=True)\n",
        "mnist_train = dataset[\"train\"].prefetch(1)\n",
        "#model = keras.models.Sequential([...])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\")\n",
        "#model.fit(mnist_train, epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 8) for input Tensor(\"input_1:0\", shape=(None, 8), dtype=float32), but it was called on an input with incompatible shape (None, 28, 28, 1).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 8) for input Tensor(\"input_1:0\", shape=(None, 8), dtype=float32), but it was called on an input with incompatible shape (None, 28, 28, 1).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-239-d0f44f6980a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#model = keras.models.Sequential([...])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sparse_categorical_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sgd\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    821\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    696\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 697\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2855\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2856\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:747 train_step\n        y_pred = self(x, training=True)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:985 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py:386 call\n        inputs, training=training, mask=mask)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py:517 _run_internal_graph\n        assert x_id in tensor_dict, 'Could not compute output ' + str(x)\n\n    AssertionError: Could not compute output Tensor(\"dense_14/BiasAdd:0\", shape=(None, 1), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx1b_6BGw7AV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}